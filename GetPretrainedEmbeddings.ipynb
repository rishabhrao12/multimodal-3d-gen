{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhrao/Documents/VSCode/multimodal-3d-gen/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from alignment import *\n",
    "import time\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dinov2 Loaded Successfully!\n",
      "CLIP Model Loaded Successfully!\n",
      "All Models loaded succesfully and set to eval mode\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dinov2_encoder = load_dinov2()\n",
    "    clip_encoder = load_clip()\n",
    "    dinov2_encoder.eval()\n",
    "    clip_encoder.eval()\n",
    "    print('All Models loaded succesfully and set to eval mode')\n",
    "except:\n",
    "    print('Error in Loading Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoiceEmbeddingDataset(Dataset):\n",
    "    \"\"\"Creates a paired modality dataset that returns text image and pc embedding (from pretrained encoders)\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path, embd_dir):\n",
    "        super().__init__()\n",
    "        # For Text\n",
    "        self.dataframe = pd.read_csv(dataset_path)\n",
    "        self.embed_dir = embd_dir\n",
    "        \"\"\" \n",
    "        data_dict = {\n",
    "            \"mesh_id\": all_text_emb,\n",
    "            \"text_emb\": [3, 768],\n",
    "            \"img_emb\": [4, 384],\n",
    "            \"pc_emb\": [8, 768],\n",
    "        }\n",
    "        \"\"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            idx (int): Index\n",
    "            tokenized_text (torch.Tensor): Tokenized text for CLIP (B, 77)\n",
    "            image_tensor (torch.Tensor): preprocessed image for Dinov2 (B, 3, 518, 518)\n",
    "            point_cloud (torch.Tensor): point cloud of mesh (B, 1024, 3)\n",
    "        \"\"\"\n",
    "        mesh_id = self.dataframe.loc[idx, 'fullId']\n",
    "        dict_path = os.path.join(self.embed_dir, f'{mesh_id}.pt')\n",
    "        data_dict = torch.load(dict_path)\n",
    "        # Retrieve the corresponding embedding using the index\n",
    "        text_embedding = data_dict['text_emb']\n",
    "        img_embedding = data_dict['img_emb']\n",
    "        pc_embedding = data_dict['pc_emb']\n",
    "\n",
    "        text_index = random.randint(0, text_embedding.shape[0] - 1)\n",
    "        img_index = random.randint(0, img_embedding.shape[0] - 1)\n",
    "        pc_index = random.randint(0, pc_embedding.shape[0] - 1)\n",
    "\n",
    "        # Now return the embedding (and other data if needed)\n",
    "        return idx, text_embedding[text_index], img_embedding[img_index], pc_embedding[pc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([598, 319, 361, 891, 540, 329, 555, 734]) torch.Size([8, 768]) torch.Size([8, 384]) torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"Data/ShapeNetSem/Datasets/final_template_1k.csv\"\n",
    "image_dir = \"Data/ShapeNetSem/Images/final_template_1k/\"\n",
    "depth_dir = \"Data/ProcessedData/final_template_1k_dmaps/\"\n",
    "embd_dir = \"Embeddings/PRETRAINED/final_template_1k/\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = ChoiceEmbeddingDataset(dataset_path, embd_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    idx, text_embd, img_embd, pc_embd = batch\n",
    "    print(idx, text_embd.shape, img_embd.shape, pc_embd.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
