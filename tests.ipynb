{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhrao/Documents/VSCode/multimodal-3d-gen/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from alignment import *\n",
    "from rag import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_path = \"TrainedModels/ALIGN/final_template_30cat_fixed/100.pth\" # \"TrainedModels/ALIGN/final_template_30cat_fixed/100.pth\" #\"TrainedModels/ALIGN/subset_200_direct_new_loss_fixed_all/140.pth\"\n",
    "rag_path = \"TrainedModels/RAG/final_template_30cat/800.pth\" # \"TrainedModels/RAG/four_hidden_val/1000.pth\"\n",
    "dataset_path = \"Data/ShapeNetSem/Datasets/final_template_30cat.csv\" # \"Data/ShapeNetSem/Datasets/subset_template_200.csv\"\n",
    "img_dir = \"Data/ShapeNetSem/Images/final_template_30cat/\" # \"Data/ShapeNetSem/Images/subset_200/\"\n",
    "pc_dir = \"Data/ProcessedData/final_template_30cat_pc/\" # \"Data/ProcessedData/PointClouds/\"\n",
    "mesh_dir = \"Data/ShapeNetSem/Files/models-OBJ/models/\"\n",
    "embed_path = f\"Embeddings/ALIGN/final_template_30cat_fixed.pt\" # f\"Embeddings/ALIGN/subset_template_new_loss_fixed_all.pt\" # f\"Embeddings/ALIGN/subset_template_200.pt\"\n",
    "frontend_dir = \"/Users/rishabhrao/Documents/VSCode/multimodal-3d-gen/env/lib/python3.12/site-packages/streamlit_3d/frontend/\"\n",
    "align_embd = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeUserInputComplexTrial(nn.Module):\n",
    "    def __init__(self, align_path=\"TrainedModels/Baseline/150.pth\", align_embd=400):\n",
    "        super().__init__()\n",
    "        self.align_path = align_path\n",
    "        self.align_embd = align_embd\n",
    "\n",
    "        self.clip_encoder = None\n",
    "\n",
    "        # Loading models\n",
    "        self.load_models()\n",
    "\n",
    "        # Preprocessing functions\n",
    "        self.tokenizer = open_clip.tokenize\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((518, 518)),  # Resize to DINO's expected input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # DINOv2 normalization\n",
    "        ])\n",
    "        self.pclip_preprocess = image_transform(\n",
    "            self.clip_encoder.visual.image_size,  # Correct image size for CLIP\n",
    "            is_train=False  # Ensures we use inference preprocessing\n",
    "        )\n",
    "\n",
    "    def preprocess_text(self, text_prompt):\n",
    "        max_length = 77\n",
    "        tokenized_text = self.tokenizer([text_prompt])\n",
    "        #print(tokenized_text.shape)\n",
    "        return tokenized_text\n",
    "\n",
    "    def preprocess_img(self, img_prompt):\n",
    "        image_tensor = self.transform(img_prompt).unsqueeze(0)\n",
    "        #print(image_tensor.shape)\n",
    "        return image_tensor\n",
    "\n",
    "    def preprocess_pc(self, pc_prompt):\n",
    "        num_points = 1024\n",
    "        indices = np.random.choice(pc_prompt.shape[0], num_points, replace=False)\n",
    "        # Sample the selected points\n",
    "        sampled_pc = pc_prompt[indices]\n",
    "        depth_maps = get_all_canonical_dmaps(sampled_pc)  # Returns list of 6 PIL Images\n",
    "        # Preprocess each depth map (e.g., Resize, ToTensor, Normalize, etc.)\n",
    "        preprocessed_maps = [self.pclip_preprocess(dmap).unsqueeze(0) for dmap in depth_maps]  # Each is (1, 3, H, W)\n",
    "        # Stack them into (Views, 3, H, W)\n",
    "        preprocessed_maps = torch.cat(preprocessed_maps, dim=0)  # Shape: (Views, 3, H, W)\n",
    "        return preprocessed_maps\n",
    "\n",
    "    def preprocess_input(self, prompt, modality):\n",
    "        if modality == \"text\":\n",
    "            processed_output = self.preprocess_text(prompt)\n",
    "        elif modality == \"img\":\n",
    "            processed_output = self.preprocess_img(prompt)\n",
    "        else:\n",
    "            processed_output = self.preprocess_pc(prompt)\n",
    "        return processed_output\n",
    "    \n",
    "    def load_models(self):\n",
    "        try:\n",
    "            self.dinov2_encoder = load_dinov2()\n",
    "            self.clip_encoder = load_clip()\n",
    "            self.pclip_encoder = load_point_clip()\n",
    "            self.align_model = load_complex_alignment(self.align_path, self.align_embd)\n",
    "            self.dinov2_encoder.eval()\n",
    "            self.clip_encoder.eval()\n",
    "            self.pclip_encoder.eval()\n",
    "            self.align_model.eval()\n",
    "        except Exception as e:\n",
    "            print(f'Error in Loading Models {e}')\n",
    "    \n",
    "    def get_projection(self, prompt, modality):\n",
    "        preprocessed_prompt = self.preprocess_input(prompt, modality)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if modality == \"text\":\n",
    "                embedding = self.clip_encoder.encode_text(preprocessed_prompt)\n",
    "                projection = self.align_model.text_proj_head(embedding)\n",
    "            elif modality == \"img\":\n",
    "                embedding = self.dinov2_encoder(preprocessed_prompt)\n",
    "                projection = self.align_model.img_proj_head(embedding)\n",
    "            else:\n",
    "                V, C, H, W = preprocessed_prompt.shape\n",
    "                # Flatten batch and view dimensions to feed into the encoder\n",
    "                flattened_dmaps = preprocessed_prompt.view(V, C, H, W)\n",
    "                with torch.no_grad():\n",
    "                    # Encode all images at once: shape (B * V, Embed)\n",
    "                    encoded_views = self.pclip_encoder.encode_image(flattened_dmaps)\n",
    "                # Reshape back to (B, V, Embed)\n",
    "                encoded_views = encoded_views.view(V, -1)\n",
    "                embedding = encoded_views.mean(dim=0).unsqueeze(0)\n",
    "                projection = self.align_model.pc_proj_head(embedding)\n",
    "\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dinov2 Loaded Successfully!\n",
      "CLIP Model Loaded Successfully!\n",
      "Point CLIP Model Loaded Successfully!\n",
      "ALIGN Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "encoder = EncodeUserInputComplexTrial(align_path=align_path, align_embd=align_embd)\n",
    "cmr = CrossModalRetrival(dataset_path, embed_path)\n",
    "rag_decoder = load_rag(checkpoint_path=rag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignedModalityDatasetNoPreprocessing(Dataset):\n",
    "    \"\"\"Creates a paired modality dataset that returns text prompt, image and 3D mesh using index\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path, image_dir, pc_dir):\n",
    "        super().__init__()\n",
    "        # For Text\n",
    "        self.dataframe = pd.read_csv(dataset_path)\n",
    "\n",
    "        # For image\n",
    "        self.image_dir = image_dir\n",
    "        self.mesh_ids = self.dataframe['fullId'].to_list()\n",
    "\n",
    "        # For PC\n",
    "        self.pc_dir = pc_dir\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            idx (int): Index\n",
    "            tokenized_text (torch.Tensor): Tokenized text for CLIP (B, 77)\n",
    "            image_tensor (torch.Tensor): preprocessed image for Dinov2 (B, 3, 518, 518)\n",
    "            point_cloud (torch.Tensor): point cloud of mesh (B, 1024, 3)\n",
    "        \"\"\"\n",
    "        mesh_id = self.dataframe.loc[idx, 'fullId']\n",
    "\n",
    "        # Choose a random template description\n",
    "        templates = self.dataframe.loc[idx, ['template1_desc', 'template2_desc', 'template3_desc']]\n",
    "        text_prompt = random.choice(templates.dropna().tolist())  # Drop NaN values safely\n",
    "\n",
    "        # Get image views        \n",
    "        image_views_dir = os.path.join(self.image_dir, mesh_id)\n",
    "        image_views = [os.path.join(image_views_dir, f) for f in os.listdir(image_views_dir) if os.path.isfile(os.path.join(image_views_dir, f))]\n",
    "        \n",
    "        # If no image views\n",
    "        if not image_views:\n",
    "            print(f\"No views for for {image_views_dir} returning empty tensors\")\n",
    "            return idx, mesh_id, torch.zeros((3, 518, 518))\n",
    "        \n",
    "        # Select one view from all\n",
    "        image_path = random.choice(image_views)\n",
    "        #image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Load point cloud\n",
    "        point_cloud = np.load(os.path.join(self.pc_dir, f\"{mesh_id}.npy\"))\n",
    "        print(point_cloud.shape)\n",
    "        \n",
    "        return idx, text_prompt, image_path, torch.from_numpy(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_mesh_ids_from_projection(projection, input_modality, output_modality, cmr):\n",
    "    idx, mesh_ids, arrays = cmr.retrieve(projection, input_modality, output_modality, top_k=5)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(category, retrieved_cats):\n",
    "    count = retrieved_cats.count(category)\n",
    "    acc = count/len(retrieved_cats)\n",
    "    return acc\n",
    "\n",
    "def update_category_accuracies(actual_category, retrieved_categories, category_accuracies):\n",
    "    # Calculate the accuracy for this sample\n",
    "    accuracy = calculate_accuracy(actual_category, retrieved_categories)\n",
    "    \n",
    "    # Check if the category is already in the dictionary\n",
    "    if actual_category in category_accuracies:\n",
    "        # If it exists, extend the list of accuracies\n",
    "        category_accuracies[actual_category].append(accuracy)\n",
    "    else:\n",
    "        # If it doesn't exist, add a new entry with the current accuracy in a list\n",
    "        category_accuracies[actual_category] = [accuracy]\n",
    "    \n",
    "    return category_accuracies\n",
    "\n",
    "def calculate_mean_accuracies(category_accuracies):\n",
    "    # Iterate over each category and calculate the mean accuracy\n",
    "    mean_accuracies = {category: np.mean(accuracies) for category, accuracies in category_accuracies.items()}\n",
    "    return mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_categories(text_proj, img_proj, pc_proj, act_cat, ca_tt, ca_ti, ca_tp, ca_ii, ca_ip, ca_pp):\n",
    "    text_text = get_aligned_mesh_ids_from_projection(text_proj, 'text', 'text', cmr)\n",
    "    text_text = dataframe.iloc[text_text]['category'].to_list()\n",
    "\n",
    "    text_img = get_aligned_mesh_ids_from_projection(text_proj, 'text', 'img', cmr)\n",
    "    text_img = dataframe.iloc[text_img]['category'].to_list()\n",
    "\n",
    "    text_pc = get_aligned_mesh_ids_from_projection(text_proj, 'text', 'pc', cmr)\n",
    "    text_pc = dataframe.iloc[text_pc]['category'].to_list()\n",
    "\n",
    "    img_img = get_aligned_mesh_ids_from_projection(text_proj, 'img', 'img', cmr)\n",
    "    img_img = dataframe.iloc[img_img]['category'].to_list()\n",
    "\n",
    "    img_pc = get_aligned_mesh_ids_from_projection(text_proj, 'img', 'pc', cmr)\n",
    "    img_pc = dataframe.iloc[img_pc]['category'].to_list()\n",
    "\n",
    "    pc_pc = get_aligned_mesh_ids_from_projection(text_proj, 'pc', 'pc', cmr)\n",
    "    pc_pc = dataframe.iloc[pc_pc]['category'].to_list()\n",
    "\n",
    "    ca_tt = update_category_accuracies(act_cat, text_text, ca_tt)\n",
    "    ca_ti = update_category_accuracies(act_cat, text_img, ca_ti)\n",
    "    ca_tp = update_category_accuracies(act_cat, text_pc, ca_tp)\n",
    "    ca_ii = update_category_accuracies(act_cat, img_img, ca_ii)\n",
    "    ca_ip = update_category_accuracies(act_cat, img_pc, ca_ip)\n",
    "    ca_pp = update_category_accuracies(act_cat, pc_pc, ca_pp)\n",
    "    return ca_tt, ca_ti, ca_tp, ca_ii, ca_ip, ca_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 3)\n",
      "(2048, 3)\n",
      "(2048, 3)\n",
      "(2048, 3)\n",
      "(2048, 3)\n",
      "(2048, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = AlignedModalityDatasetNoPreprocessing(dataset_path, img_dir, pc_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "ca_tt, ca_ti, ca_tp, ca_ii, ca_ip, ca_pp = {}, {}, {}, {}, {}, {}\n",
    "for i, batch in enumerate(dataloader):\n",
    "    idx, text_prompt, image_path, pc = batch\n",
    "    actual_category = dataframe.iloc[idx]['category'].iloc[0]\n",
    "    #print(idx, text_prompt, image_path, pc.shape)\n",
    "    image = Image.open(image_path[0]).convert('RGB')\n",
    "    text_proj = encoder.get_projection(text_prompt[0], 'text')\n",
    "    img_proj = encoder.get_projection(image, 'img')\n",
    "    pc_proj = encoder.get_projection(pc[0], 'pc')\n",
    "    ca_tt, ca_ti, ca_tp, ca_ii, ca_ip, ca_pp = get_retrieved_categories(text_proj, img_proj, pc_proj, actual_category, ca_tt, ca_ti, ca_tp, ca_ii, ca_ip, ca_pp)\n",
    "    if i==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_tt = calculate_mean_accuracies(ca_tt)\n",
    "ca_ti = calculate_mean_accuracies(ca_ti)\n",
    "ca_tp = calculate_mean_accuracies(ca_tp)\n",
    "ca_ii = calculate_mean_accuracies(ca_ii)\n",
    "ca_ip = calculate_mean_accuracies(ca_ip)\n",
    "ca_pp = calculate_mean_accuracies(ca_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Refrigerator': np.float64(1.0),\n",
       "  'Bench': np.float64(1.0),\n",
       "  'Couch': np.float64(1.0),\n",
       "  'TV': np.float64(0.6),\n",
       "  'PersonStanding': np.float64(1.0)},\n",
       " {'Refrigerator': np.float64(0.2),\n",
       "  'Bench': np.float64(1.0),\n",
       "  'Couch': np.float64(1.0),\n",
       "  'TV': np.float64(0.8),\n",
       "  'PersonStanding': np.float64(1.0)},\n",
       " {'Refrigerator': np.float64(0.0),\n",
       "  'Bench': np.float64(0.30000000000000004),\n",
       "  'Couch': np.float64(0.6),\n",
       "  'TV': np.float64(0.0),\n",
       "  'PersonStanding': np.float64(1.0)},\n",
       " {'Refrigerator': np.float64(0.2),\n",
       "  'Bench': np.float64(1.0),\n",
       "  'Couch': np.float64(1.0),\n",
       "  'TV': np.float64(0.8),\n",
       "  'PersonStanding': np.float64(1.0)},\n",
       " {'Refrigerator': np.float64(0.0),\n",
       "  'Bench': np.float64(0.30000000000000004),\n",
       "  'Couch': np.float64(0.6),\n",
       "  'TV': np.float64(0.0),\n",
       "  'PersonStanding': np.float64(1.0)},\n",
       " {'Refrigerator': np.float64(0.0),\n",
       "  'Bench': np.float64(0.30000000000000004),\n",
       "  'Couch': np.float64(0.6),\n",
       "  'TV': np.float64(0.0),\n",
       "  'PersonStanding': np.float64(1.0)})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_tt, ca_ti, ca_tp, ca_ii, ca_ip, ca_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(category_accuracies.items()), columns=['Category', 'Accuracy'])\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('category_accuracies.csv', index=False)\n",
    "\n",
    "print(\"CSV file has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "read_dir = \"Accuracy/final_template_30cat_fixed/\"\n",
    "# List your CSV file paths here\n",
    "csv_files = [\"text_text.csv\", \"text_img.csv\", \"text_pc.csv\", \"img_img.csv\", \"img_pc.csv\", \"pc_pc.csv\"]\n",
    "\n",
    "# List to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Manually specify new accuracy column names for each file\n",
    "accuracy_column_names = ['text_text', 'text_img', 'text_pc', 'img_img', 'img_pc', 'pc_pc']  # Modify as needed\n",
    "\n",
    "# Iterate over files\n",
    "for i, csv_file in enumerate(csv_files):\n",
    "    file_path = os.path.join(read_dir, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Rename the accuracy column\n",
    "    df.rename(columns={'Accuracy': accuracy_column_names[i]}, inplace=True)\n",
    "    df[accuracy_column_names[i]] = df[accuracy_column_names[i]].round(2)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all dataframes on the 'category' column\n",
    "combined_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    combined_df = pd.merge(combined_df, df, on='Category')\n",
    "\n",
    "# Save the combined dataframe to a new CSV\n",
    "save_path = os.path.join(read_dir, \"accuracy.csv\")\n",
    "combined_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
