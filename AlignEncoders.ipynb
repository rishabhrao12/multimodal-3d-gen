{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor, CLIPTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import AlignedModalityDataset\n",
    "from open_clip import image_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([55])\n",
      "Tokenized Text: torch.Size([1, 77]), <class 'torch.Tensor'>\n",
      "Image Tensor: torch.Size([1, 3, 518, 518]), <class 'torch.Tensor'>\n",
      "Point Cloud: torch.Size([1, 1024, 3]), <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"Data/ShapeNetSem/Datasets/subset_template_200.csv\"\n",
    "image_dir = \"Data/ShapeNetSem/Images/subset_200\"\n",
    "pc_dir = \"Data/ProcessedData/PointClouds\"\n",
    "\n",
    "dataset = AlignedModalityDataset(dataset_path, image_dir, pc_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for idx, token_text, image_tensor, point_cloud in dataloader:\n",
    "    print(idx)\n",
    "    print(f\"Tokenized Text: {token_text.shape}, {type(token_text)}\")\n",
    "    print(f\"Image Tensor: {image_tensor.shape}, {type(image_tensor)}\")\n",
    "    print(f\"Point Cloud: {point_cloud.shape}, {type(point_cloud)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def point_cloud_to_depth_map(pc_data):\n",
    "    \"\"\"Converts a 3D point cloud to a 2D depth image.\"\"\"\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pc_data.numpy())\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(visible=False)\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.8)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "    # Capture depth image\n",
    "    depth = vis.capture_depth_float_buffer(True)\n",
    "    vis.destroy_window()\n",
    "\n",
    "    # Convert depth to PIL Image\n",
    "    depth_np = np.asarray(depth)\n",
    "    depth_image = Image.fromarray((depth_np * 255).astype(np.uint8))\n",
    "\n",
    "    return depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "def load_dinov2():\n",
    "    # Load Pretrained DINOv2 Model (Use 'vit_small_patch14_dinov2' for smaller models)\n",
    "    model_path = \"PretrainedModels/dinov2_vits14_pretrain.pth\"  # Change to your local path\n",
    "    model = timm.create_model(\"vit_small_patch14_dinov2\", pretrained=False)\n",
    "\n",
    "    # Load the state dictionary and remove \"mask_token\"\n",
    "    checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "    checkpoint = {k: v for k, v in checkpoint.items() if k != \"mask_token\"}  # Remove unexpected key\n",
    "\n",
    "    # Load the modified state dict into the model\n",
    "    model.load_state_dict(checkpoint, strict=False)  # strict=False allows minor mismatches\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print('Dinov2 Loaded Successfully!')\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_clip():\n",
    "    model_name = \"ViT-L-14\"  # Change this to ViT-L/14 if needed\n",
    "    save_path = f\"PretrainedModels/clip_vitl14_pretrain.pth\"\n",
    "\n",
    "    clip_model = open_clip.create_model(model_name, pretrained=False)\n",
    "\n",
    "    # Load saved state dict\n",
    "    checkpoint = torch.load(save_path, map_location=\"cpu\")\n",
    "    clip_model.load_state_dict(checkpoint)\n",
    "    clip_model.eval()\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model.to(device)\n",
    "    print(\"CLIP Model Loaded Successfully!\")\n",
    "    return clip_model\n",
    "\n",
    "def load_point_clip():\n",
    "    save_path = \"PretrainedModels/clip_vitl14_pretrain.pth\"\n",
    "    model_name = \"ViT-L-14\"\n",
    "\n",
    "    # Load Model Without Downloading\n",
    "    clip_model = open_clip.create_model(model_name, pretrained=False)\n",
    "    checkpoint = torch.load(save_path, map_location=\"cpu\")\n",
    "    clip_model.load_state_dict(checkpoint)\n",
    "    clip_model.eval()\n",
    "\n",
    "    # Move Model to GPU if Available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model.to(device)\n",
    "\n",
    "    print(\"Point CLIP Model Loaded Successfully!\")\n",
    "    return clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dinov2 Loaded Successfully\n",
      "CLIP Model Loaded Successfully!\n",
      "Point CLIP Model Loaded Successfully!\n",
      "All Models loaded succesfully and set to eval mode\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dinov2_encoder = load_dinov2()\n",
    "    clip_encoder = load_clip()\n",
    "    pclip_encoder = load_point_clip()\n",
    "    dinov2_encoder.eval()\n",
    "    clip_encoder.eval()\n",
    "    pclip_encoder.eval()\n",
    "    print('All Models loaded succesfully and set to eval mode')\n",
    "except:\n",
    "    print('Error in Loading Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        \"\"\"\n",
    "        Computes NT-Xent loss for a batch of paired embeddings.\n",
    "        - z1: First modality (e.g., text)\n",
    "        - z2: Second modality (e.g., pointcloud)\n",
    "        \"\"\"\n",
    "        batch_size = z1.shape[0]\n",
    "\n",
    "        # Normalize embeddings\n",
    "        z1 = F.normalize(z1, dim=-1)\n",
    "        z2 = F.normalize(z2, dim=-1)\n",
    "        #print(z1.shape, z2.shape)\n",
    "        # Compute cosine similarity matrix\n",
    "        similarity = torch.mm(z1, z2.T) / self.temperature  # Shape: [batch_size, batch_size]\n",
    "        #print(similarity.shape)\n",
    "        # Labels should be in range [0, batch_size-1]\n",
    "        labels = torch.arange(batch_size, device=z1.device)  # Correct labels\n",
    "        #print(labels)\n",
    "        # Compute contrastive loss using cross-entropy\n",
    "        loss = F.cross_entropy(similarity, labels)\n",
    "\n",
    "        # Debugging output\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            print(f\"Mean Similarity: {similarity.mean().item():.4f}, Loss: {loss.item():.4f}\")\n",
    "        \"\"\"\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 2*in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*in_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "class AlignEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.text_proj_head = MLP(768, embed_dim)\n",
    "        self.img_proj_head = MLP(384, embed_dim)\n",
    "        self.pc_proj_head = MLP(768, embed_dim)\n",
    "    \n",
    "    def forward(self, text, img, pc):\n",
    "        text_proj = self.text_proj_head(text)\n",
    "        img_proj = self.img_proj_head(img)\n",
    "        pc_proj = self.pc_proj_head(pc)\n",
    "\n",
    "        return text_proj, img_proj, pc_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0735, grad_fn=<DivBackward0>)\n",
      "tensor(2.0871, grad_fn=<DivBackward0>)\n",
      "tensor(2.0751, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m point_cloud\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Convert each point cloud to a depth map and preprocess it\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m depth_maps \u001b[38;5;241m=\u001b[39m [preprocess(\u001b[43mpoint_cloud_to_depth_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_cloud\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Stack depth maps into a single batch tensor\u001b[39;00m\n\u001b[1;32m     38\u001b[0m depth_maps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(depth_maps, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Shape: [batch_size, 3, H, W]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m, in \u001b[0;36mpoint_cloud_to_depth_map\u001b[0;34m(pc_data)\u001b[0m\n\u001b[1;32m     15\u001b[0m vis\u001b[38;5;241m.\u001b[39mupdate_renderer()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Capture depth image\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_depth_float_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m vis\u001b[38;5;241m.\u001b[39mdestroy_window()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert depth to PIL Image\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "dataset_path = \"Data/ShapeNetSem/Datasets/subset_template_200.csv\"\n",
    "image_dir = \"Data/ShapeNetSem/Images/subset_200\"\n",
    "pc_dir = \"Data/ProcessedData/PointClouds\"\n",
    "\n",
    "# Set up CLIP preprocessing\n",
    "preprocess = image_transform(\n",
    "    clip_encoder.visual.image_size,  # Correct image size for CLIP\n",
    "    is_train=False  # Ensures we use inference preprocessing\n",
    ")\n",
    "\n",
    "dataset = AlignedModalityDataset(dataset_path, image_dir, pc_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "align_model = AlignEncoder(400)\n",
    "align_model.to(device)\n",
    "\n",
    "loss_fn = NTXentLoss(temperature=0.07)\n",
    "optimizer = torch.optim.Adam(align_model.parameters(), lr=1e-4)\n",
    "\n",
    "all_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        idx, tokenized_text, image_tensor, point_cloud = batch\n",
    "        tokenized_text = tokenized_text.to(device) # (B, 77)\n",
    "        image_tensor = image_tensor.to(device) # (B, 3, 518, 518)\n",
    "        \n",
    "        point_cloud = point_cloud.to(device) # (B, 1024, 3)\n",
    "\n",
    "        #print(tokenized_text.shape, image_tensor.shape, point_cloud.shape)\n",
    "\n",
    "        # Assuming point_cloud is a batch of point clouds (shape: [batch_size, N, 3])\n",
    "        batch_size = point_cloud.shape[0]\n",
    "\n",
    "        # Convert each point cloud to a depth map and preprocess it\n",
    "        depth_maps = [preprocess(point_cloud_to_depth_map(point_cloud[i])).unsqueeze(0) for i in range(batch_size)]\n",
    "\n",
    "        # Stack depth maps into a single batch tensor\n",
    "        depth_maps = torch.cat(depth_maps, dim=0).to(device)  # Shape: [batch_size, 3, H, W]\n",
    "        #print(depth_maps.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_emb = clip_encoder.encode_text(tokenized_text) # (B, 768)\n",
    "            img_emb = dinov2_encoder(image_tensor) # (B, 384)\n",
    "            pc_emb = pclip_encoder.encode_image(depth_maps) # (B, 768)\n",
    "\n",
    "        #print(text_emb.shape, img_emb.shape, pc_emb.shape)\n",
    "        text_proj, img_proj, pc_proj = align_model(text_emb, img_emb, pc_emb)\n",
    "        #print(text_proj.shape, img_proj.shape, pc_proj.shape)\n",
    "        loss_text_point = loss_fn(text_proj, pc_proj)\n",
    "        loss_text_image = loss_fn(text_proj, img_proj)\n",
    "        loss_image_point = loss_fn(img_proj, pc_proj)\n",
    "\n",
    "        #print('Loss: ', loss_text_point, loss_text_image, loss_image_point)\n",
    "        avg_loss = (loss_text_point + loss_text_image + loss_image_point) / 3\n",
    "        optimizer.zero_grad()\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(avg_loss.item())\n",
    "\n",
    "    avg_epoch_loss = sum(epoch_losses)/len(epoch_losses)\n",
    "    all_losses.append(avg_epoch_loss)\n",
    "    print(f'Epoch {epoch} loss: {sum(avg_epoch_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
