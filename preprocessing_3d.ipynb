{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farthest_point_sampling(points, num_samples):\n",
    "    \"\"\"Perform farthest point sampling (FPS) to get fixed number of points.\"\"\"\n",
    "    selected_pts = [points[np.random.randint(len(points))]]\n",
    "    for _ in range(num_samples - 1):\n",
    "        distances = np.linalg.norm(points - selected_pts[-1], axis=1)\n",
    "        farthest_idx = np.argmax(distances)\n",
    "        selected_pts.append(points[farthest_idx])\n",
    "    return np.array(selected_pts)\n",
    "\n",
    "def preprocess_and_save(obj_filepaths, output_dir, num_points=1024):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_path in obj_filepaths:\n",
    "        mesh = trimesh.load_mesh(file_path)\n",
    "        \n",
    "        # Try sampling points\n",
    "        points, _ = trimesh.sample.sample_surface_even(mesh, num_points)\n",
    "        \n",
    "        # If not enough points, pad with random duplicates\n",
    "        if len(points) < num_points:\n",
    "            indices = np.random.choice(len(points), num_points - len(points), replace=True)\n",
    "            extra_points = points[indices]\n",
    "            points = np.vstack([points, extra_points])\n",
    "        \n",
    "        # If too many points, use FPS to downsample\n",
    "        elif len(points) > num_points:\n",
    "            points = farthest_point_sampling(points, num_points)\n",
    "        \n",
    "        # Normalize the point cloud\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        points -= centroid\n",
    "        scale = np.max(np.linalg.norm(points, axis=1))\n",
    "        points /= scale\n",
    "        \n",
    "        # Save the fixed-size point cloud\n",
    "        filename = os.path.splitext(os.path.basename(file_path))[0] + \".npy\"\n",
    "        np.save(os.path.join(output_dir, filename), points)\n",
    "\n",
    "    print(f\"Processed {len(obj_filepaths)} meshes and saved point clouds to {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/ShapeNetSem/Datasets/subset_template_200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_ids = df['fullId'].to_list()\n",
    "mesh_dir = 'Data/ShapeNetSem/Files/models-OBJ/models/'\n",
    "mesh_paths = [f'{mesh_dir}{id}.obj' for id in mesh_ids if os.path.isfile(f'{mesh_dir}{id}.obj')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(mesh_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 477/1024 samples!\n",
      "only got 1023/1024 samples!\n",
      "only got 772/1024 samples!\n",
      "only got 961/1024 samples!\n",
      "only got 921/1024 samples!\n",
      "only got 981/1024 samples!\n",
      "only got 435/1024 samples!\n",
      "only got 337/1024 samples!\n",
      "only got 477/1024 samples!\n",
      "only got 683/1024 samples!\n",
      "only got 592/1024 samples!\n",
      "only got 487/1024 samples!\n",
      "only got 407/1024 samples!\n",
      "only got 303/1024 samples!\n",
      "only got 117/1024 samples!\n",
      "only got 415/1024 samples!\n",
      "only got 216/1024 samples!\n",
      "only got 203/1024 samples!\n",
      "only got 421/1024 samples!\n",
      "only got 938/1024 samples!\n",
      "only got 97/1024 samples!\n",
      "only got 92/1024 samples!\n",
      "only got 331/1024 samples!\n",
      "only got 660/1024 samples!\n",
      "only got 907/1024 samples!\n",
      "only got 127/1024 samples!\n",
      "only got 662/1024 samples!\n",
      "only got 818/1024 samples!\n",
      "only got 621/1024 samples!\n",
      "only got 400/1024 samples!\n",
      "only got 422/1024 samples!\n",
      "only got 856/1024 samples!\n",
      "only got 591/1024 samples!\n",
      "only got 902/1024 samples!\n",
      "only got 309/1024 samples!\n",
      "only got 635/1024 samples!\n",
      "only got 574/1024 samples!\n",
      "only got 564/1024 samples!\n",
      "only got 470/1024 samples!\n",
      "only got 859/1024 samples!\n",
      "only got 737/1024 samples!\n",
      "only got 372/1024 samples!\n",
      "only got 266/1024 samples!\n",
      "only got 372/1024 samples!\n",
      "only got 546/1024 samples!\n",
      "only got 297/1024 samples!\n",
      "only got 460/1024 samples!\n",
      "only got 401/1024 samples!\n",
      "only got 374/1024 samples!\n",
      "only got 971/1024 samples!\n",
      "only got 471/1024 samples!\n",
      "only got 493/1024 samples!\n",
      "only got 284/1024 samples!\n",
      "only got 299/1024 samples!\n",
      "only got 348/1024 samples!\n",
      "only got 398/1024 samples!\n",
      "only got 353/1024 samples!\n",
      "only got 822/1024 samples!\n",
      "only got 291/1024 samples!\n",
      "only got 219/1024 samples!\n",
      "only got 257/1024 samples!\n",
      "only got 642/1024 samples!\n",
      "only got 171/1024 samples!\n",
      "only got 270/1024 samples!\n",
      "only got 736/1024 samples!\n",
      "only got 517/1024 samples!\n",
      "only got 526/1024 samples!\n",
      "only got 554/1024 samples!\n",
      "only got 550/1024 samples!\n",
      "only got 276/1024 samples!\n",
      "only got 239/1024 samples!\n",
      "only got 310/1024 samples!\n",
      "only got 915/1024 samples!\n",
      "only got 595/1024 samples!\n",
      "only got 493/1024 samples!\n",
      "only got 332/1024 samples!\n",
      "only got 268/1024 samples!\n",
      "only got 732/1024 samples!\n",
      "only got 204/1024 samples!\n",
      "only got 292/1024 samples!\n",
      "only got 648/1024 samples!\n",
      "only got 626/1024 samples!\n",
      "only got 524/1024 samples!\n",
      "only got 451/1024 samples!\n",
      "only got 488/1024 samples!\n",
      "only got 629/1024 samples!\n",
      "only got 482/1024 samples!\n",
      "only got 412/1024 samples!\n",
      "only got 542/1024 samples!\n",
      "only got 335/1024 samples!\n",
      "only got 419/1024 samples!\n",
      "only got 212/1024 samples!\n",
      "only got 308/1024 samples!\n",
      "only got 816/1024 samples!\n",
      "only got 239/1024 samples!\n",
      "only got 496/1024 samples!\n",
      "only got 305/1024 samples!\n",
      "only got 576/1024 samples!\n",
      "only got 328/1024 samples!\n",
      "only got 281/1024 samples!\n",
      "only got 601/1024 samples!\n",
      "only got 626/1024 samples!\n",
      "only got 285/1024 samples!\n",
      "only got 602/1024 samples!\n",
      "only got 205/1024 samples!\n",
      "only got 367/1024 samples!\n",
      "only got 656/1024 samples!\n",
      "only got 230/1024 samples!\n",
      "only got 237/1024 samples!\n",
      "only got 394/1024 samples!\n",
      "only got 259/1024 samples!\n",
      "only got 288/1024 samples!\n",
      "only got 139/1024 samples!\n",
      "only got 252/1024 samples!\n",
      "only got 735/1024 samples!\n",
      "only got 324/1024 samples!\n",
      "only got 423/1024 samples!\n",
      "only got 284/1024 samples!\n",
      "only got 705/1024 samples!\n",
      "only got 914/1024 samples!\n",
      "only got 924/1024 samples!\n",
      "only got 304/1024 samples!\n",
      "only got 223/1024 samples!\n",
      "only got 392/1024 samples!\n",
      "only got 447/1024 samples!\n",
      "only got 412/1024 samples!\n",
      "only got 746/1024 samples!\n",
      "only got 446/1024 samples!\n",
      "only got 463/1024 samples!\n",
      "only got 346/1024 samples!\n",
      "only got 480/1024 samples!\n",
      "only got 501/1024 samples!\n",
      "only got 410/1024 samples!\n",
      "only got 418/1024 samples!\n",
      "only got 547/1024 samples!\n",
      "only got 561/1024 samples!\n",
      "only got 557/1024 samples!\n",
      "only got 987/1024 samples!\n",
      "only got 545/1024 samples!\n",
      "only got 762/1024 samples!\n",
      "only got 581/1024 samples!\n",
      "only got 649/1024 samples!\n",
      "only got 533/1024 samples!\n",
      "only got 700/1024 samples!\n",
      "only got 650/1024 samples!\n",
      "only got 388/1024 samples!\n",
      "only got 356/1024 samples!\n",
      "only got 328/1024 samples!\n",
      "only got 445/1024 samples!\n",
      "only got 629/1024 samples!\n",
      "only got 381/1024 samples!\n",
      "only got 286/1024 samples!\n",
      "only got 277/1024 samples!\n",
      "only got 351/1024 samples!\n",
      "only got 274/1024 samples!\n",
      "only got 661/1024 samples!\n",
      "only got 874/1024 samples!\n",
      "only got 813/1024 samples!\n",
      "only got 904/1024 samples!\n",
      "only got 333/1024 samples!\n",
      "only got 251/1024 samples!\n",
      "only got 443/1024 samples!\n",
      "only got 605/1024 samples!\n",
      "only got 480/1024 samples!\n",
      "only got 585/1024 samples!\n",
      "only got 462/1024 samples!\n",
      "only got 584/1024 samples!\n",
      "only got 497/1024 samples!\n",
      "only got 898/1024 samples!\n",
      "only got 709/1024 samples!\n",
      "only got 771/1024 samples!\n",
      "only got 1014/1024 samples!\n",
      "only got 239/1024 samples!\n",
      "only got 324/1024 samples!\n",
      "only got 408/1024 samples!\n",
      "only got 381/1024 samples!\n",
      "only got 363/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 meshes and saved point clouds to Data/ProcessedData/PointClouds\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"Data/ProcessedData/PointClouds\"\n",
    "preprocess_and_save(mesh_paths, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "def farthest_point_sampling(points, num_samples):\n",
    "    \"\"\"Performs farthest point sampling (FPS) to ensure uniform point distribution.\"\"\"\n",
    "    selected_pts = [points[np.random.randint(len(points))]]\n",
    "    for _ in range(num_samples - 1):\n",
    "        distances = np.linalg.norm(points - selected_pts[-1], axis=1)\n",
    "        farthest_idx = np.argmax(distances)\n",
    "        selected_pts.append(points[farthest_idx])\n",
    "    return np.array(selected_pts)\n",
    "\n",
    "def load_binvox_as_point_cloud(binvox_path, num_points=1024):\n",
    "    \"\"\"Load a BINVOX file and extract a point cloud using trimesh.\"\"\"\n",
    "    try:\n",
    "        # Load the binvox file as a VoxelGrid\n",
    "        voxel_grid = trimesh.exchange.binvox.load_binvox(binvox_path)\n",
    "        points = voxel_grid.points  # Extract voxel positions\n",
    "        \n",
    "        if len(points) < num_points:\n",
    "            indices = np.random.choice(len(points), num_points - len(points), replace=True)\n",
    "            points = np.vstack([points, points[indices]])\n",
    "\n",
    "        if len(points) > num_points:\n",
    "            points = farthest_point_sampling(points, num_points)\n",
    "\n",
    "        return points\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BINVOX file {binvox_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_and_save(mesh_ids, obj_dir, binvox_dir, output_dir, num_points=1024):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for mesh_id in mesh_ids:\n",
    "        obj_path = os.path.join(obj_dir, mesh_id + \".obj\")\n",
    "        binvox_path = os.path.join(binvox_dir, mesh_id + \".binvox\")\n",
    "\n",
    "        points = None\n",
    "\n",
    "        if os.path.exists(obj_path):\n",
    "            mesh = trimesh.load_mesh(obj_path)\n",
    "            points, _ = trimesh.sample.sample_surface(mesh, num_points * 4)\n",
    "\n",
    "        if (points is None or len(points) < num_points) and os.path.exists(binvox_path):\n",
    "            points = load_binvox_as_point_cloud(binvox_path, num_points)\n",
    "\n",
    "        if points is None or len(points) < num_points:\n",
    "            continue  # Skip if neither method produces enough points\n",
    "\n",
    "        if len(points) > num_points:\n",
    "            points = farthest_point_sampling(points, num_points)\n",
    "\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        points -= centroid\n",
    "        scale = np.max(np.linalg.norm(points, axis=1))\n",
    "        points /= scale\n",
    "\n",
    "        filename = mesh_id + \".npy\"\n",
    "        np.save(os.path.join(output_dir, filename), points)\n",
    "\n",
    "    print(f\"Processed {len(mesh_ids)} meshes and saved point clouds to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 meshes and saved point clouds to Data/ProcessedData/PointCloudswithBinvox\n"
     ]
    }
   ],
   "source": [
    "obj_directory = \"Data/ShapeNetSem/Files/models-OBJ/models\"\n",
    "binvox_directory = \"Data/ShapeNetSem/Files/models-binvox\"\n",
    "output_directory = \"Data/ProcessedData/PointCloudswithBinvox\"\n",
    "preprocess_and_save(mesh_ids, obj_directory, binvox_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PointBertPCModalityDataset(Dataset):\n",
    "    \"\"\"Creates text modality dataset for ShapeNetSem with CLIP Tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, pc_dir):\n",
    "        super().__init__()\n",
    "        self.dataframe = pd.read_csv(dataset_path)\n",
    "        self.pc_dir = pc_dir\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            idx (int): Index\n",
    "            tokenized_text (torch.Tensor): Tokenized text for CLIP\n",
    "            text_prompt (str): The actual text prompt\n",
    "        \"\"\"\n",
    "        mesh_id = self.dataframe.loc[idx, 'fullId']\n",
    "\n",
    "        \n",
    "\n",
    "        return idx, tokenized_text.squeeze(0), text_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
